{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681da3a6-1d50-414f-be7a-fa04af7146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ee41a0-7902-4145-8ebf-cafab737903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502713e7-47ba-4a70-a2ce-4ecfe6ae560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stops = stopwords.words('english')\n",
    "stops.extend([\"i'm\", \"he's\", \"i've\", \"i'll\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb20ab0a-9c51-4fdb-8666-cd4e620031de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(rows):\n",
    "    for row in rows:\n",
    "        doc = nlp(row['Text'])\n",
    "        row['tokens'] = []\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT' and token.is_stop is False:\n",
    "                tokens = dict()\n",
    "                tokens['word'] = token.text\n",
    "                tokens['pos'] = token.pos_\n",
    "                tokens['lemma'] = token.lemma_\n",
    "                row['tokens'].append(tokens)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73ba5ba-ce45-4311-bfb8-91f8abeef1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(rows):\n",
    "    rows = parse(rows)\n",
    "    for row in rows:\n",
    "        pos_distribution = dict()\n",
    "        for token in row['tokens']:\n",
    "            if token['pos'] in pos_distribution.keys():\n",
    "                pos_distribution[token['pos']] += 1\n",
    "            else:\n",
    "                pos_distribution[token['pos']] = 1\n",
    "        row['pos'] = pos_distribution\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db9107c-a199-4d41-88a9-8047faf0dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_len(rows):\n",
    "    for row in rows:\n",
    "        phrase = row['Text']\n",
    "        words = [w.lower() for w in word_tokenize(phrase) if w.isalpha()]\n",
    "        row['len'] = len(words)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70903fc9-a841-49be-ad11-fbc1ea11dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_order(sent):\n",
    "    d = nlp(sent)\n",
    "    roots = [token for token in d if token.head == token]\n",
    "    orders = []\n",
    "    for root in roots:\n",
    "        d = {}\n",
    "        d['V'] = root.i\n",
    "        for child in root.children:\n",
    "            if child.dep_ == 'nsubj':\n",
    "                d['S'] = child.i\n",
    "            if child.dep_== 'dobj':\n",
    "                d['O'] = child.i\n",
    "        listt = sorted(d.keys(),key=d.get)\n",
    "        order = ''.join(listt)\n",
    "        orders.append(order)\n",
    "    return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de9b714-f794-4fb8-b449-2d7296845079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotes_order(rows):\n",
    "    for row in rows:\n",
    "        row['word_order'] = word_order(row['Text'])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694aaf2f-23a6-4b6f-80ff-5f6fbdcadce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_list = '!\"«»“”#$%&\\–-–—()*+,./\\:;<=>?@[]^_`{|}~1234567890'\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    for char in text:\n",
    "        if char in punct_list:\n",
    "            text = text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b23eb745-ea9c-4e28-a280-75c03a9ecd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция находящая полезные слова\n",
    "def extract_words(text):\n",
    "    words = clean(text).split()\n",
    "    good_words = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            good_words.append(word) \n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cead469d-54d3-4ea5-8d55-c5b3e8d48978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular_words(rows):\n",
    "    txt_ps = []\n",
    "    txt_pe = []\n",
    "    txt_np = []\n",
    "    \n",
    "    populars = {'pricesses':{}, 'princes':{}, 'non-p':{}}\n",
    "\n",
    "    for row in rows:\n",
    "        # принцессы\n",
    "        if row['Speaker_Status'] == 'PRINCESS':\n",
    "            txt_ps.extend(extract_words(row['Text']))\n",
    "            \n",
    "        freq_ps = Counter(txt_ps)\n",
    "        freq_psSorted = sorted(freq_ps.items(), key = lambda x: x[1], reverse=True)\n",
    "        for i in freq_psSorted[:50]:\n",
    "            populars['pricesses'][i[0]] = freq_ps[i[0]]\n",
    "\n",
    "        # принцы\n",
    "        if row['Speaker_Status'] == 'PRINCE':\n",
    "            txt_pe.extend(extract_words(row['Text']))\n",
    "            \n",
    "            freq_pe = Counter(txt_pe)\n",
    "            freq_peSorted = sorted(freq_pe.items(), key = lambda x: x[1], reverse=True)\n",
    "            for i in freq_peSorted[:50]:\n",
    "                populars['princes'][i[0]] = freq_ps[i[0]]\n",
    "\n",
    "        # челядь\n",
    "        if row['Speaker_Status'] == 'NON-P':\n",
    "            txt_np.extend(extract_words(row['Text']))\n",
    "            \n",
    "            freq_np = Counter(txt_np)\n",
    "            freq_npSorted = sorted(freq_np.items(), key = lambda x: x[1], reverse=True)\n",
    "            for i in freq_npSorted[:50]:\n",
    "                populars['non-p'][i[0]] = freq_ps[i[0]]\n",
    "                \n",
    "    return populars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b04cee2-38d9-4dcc-b12a-1dcc8f55e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "with open('princess_corpus.csv', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        row = dict(row)\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a8bb5-ee68-47bc-affb-4a6758e2445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pos(parse(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c415f-3f92-477c-a241-ae1a2825bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = phrase_len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322b400-99c0-454f-8645-c04dd7565f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = quotes_order(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb60cd-b938-4bae-a8d1-4b705b088350",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d3bf7a-1ca4-4200-8819-a9d179a9c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "populars = popular_words(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1d4a135-fbbe-489a-85af-9dbdc7e9b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('popular_words.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(populars, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131f98ee-9619-4eeb-aca6-351731489f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pricesses': {'wishing': 6, 'find': 25, 'ah': 4, 'today': 3, 'ahahahahahh': 4, 'well': 55, 'wish': 17, 'one': 51, 'love': 48, 'hoping': 2, 'nice': 2, 'things': 7, \"he'll\": 3, 'say': 34, 'want': 52, 'know': 104, 'secret': 1, 'promise': 1, 'tell': 28, 'standing': 1, 'make': 3, \"that's\": 39, 'hear': 1, 'echoing': 1, 'soon': 1, 'come': 74, 'true': 27, 'dreaming': 1, 'ahh': 1, 'oh': 188, 'song': 9, 'mama': 2, 'papa': 21, 'goodbye': 16, 'hello': 2, \"what's\": 14, 'matter': 4, \"where's\": 1, 'believe': 1, 'lost': 1, 'please': 68, 'cry': 1, 'perk': 1, 'smile': 5, 'better': 4, \"can't\": 57, 'far': 1, 'fly': 1, 'understand': 13, 'butbut': 1, 'away': 30, 'sing': 18, 'life': 23, 'world': 15, 'along': 4, 'like': 72, 'sure': 10, 'sleep': 4, \"there's\": 9, 'little': 39, 'seven': 4, 'children': 4, 'look': 35, 'tsk': 12, 'would': 37, 'maybe': 16, 'stay': 7, 'house': 3, 'mother': 25, 'use': 2, 'fireplace': 2, 'broom': 2, 'room': 2, 'clean': 2, 'happy': 3, 'adorable': 2, 'guess': 2, 'sleepy': 3, 'mean': 31, 'must': 17, 'queen': 2, 'let': 62, 'yes': 43, 'wash': 5, 'uh': 25, 'see': 68, 'doc': 3, 'bashful': 3, 'never': 71, 'day': 33, 'grumpy': 6, 'right': 51, 'go': 85, 'heighho': 3, \"we'll\": 7, 'anew': 3, 'spring': 3, 'dreams': 13, 'dream': 22, 'heart': 5, 'get': 59, 'keep': 5, 'kitty': 4, 'breakfast': 5, 'shoo': 5, 'good': 27, 'morning': 10, 'lucifer': 8, 'sweet': 12, 'nightingale': 7, 'stepmother': 5, 'coming': 6, 'dress': 5, 'time': 33, 'prince': 28, 'thank': 27, 'think': 36, 'bruno': 8, 'wonder': 7, 'wait': 45, 'ever': 6, 'going': 27, 'sorry': 34, 'got': 39, 'could': 31, 'flounder': 8, 'daddy': 19, 'gaston': 14, 'father': 38, 'wonderful': 11, 'really': 23, 'simba': 14, 'around': 28, 'back': 43, 'something': 25, 'help': 29, 'way': 28, 'gonna': 27, 'us': 26, 'mom': 36, 'need': 20, 'okay': 32, 'elsa': 30}, 'princes': {'today': 3, 'hello': 2, 'one': 51, 'song': 9, 'love': 48, 'wait': 45, 'heart': 5, 'frighten': 1, 'please': 68, 'run': 6, 'away': 12, 'found': 1, 'hear': 3, 'say': 12, 'tenderly': 0, 'beating': 0, 'ever': 6, 'entreating': 0, 'constant': 0, 'true': 12, 'possessed': 0, 'thrilling': 0, 'keeps': 2, 'singing': 1, \"what's\": 15, 'matter': 4, 'yes': 43, \"can't\": 57, 'go': 85, 'prince': 28, 'know': 103, 'come': 74, 'back': 43, 'oh': 187, 'even': 8, 'name': 3, 'find': 25, 'samson': 0, 'beautiful': 4, \"let's\": 4, 'ohhh': 0, 'carrots': 0, 'something': 25, 'strange': 1, 'voice': 2, 'real': 0, 'maybe': 10, 'mysterious': 0, 'wood': 0, 'sprite': 0, 'upon': 3, 'dream': 12, 'gleam': 0, 'met': 5, 'father': 37, 'said': 2, 'girl': 2, 'marry': 4, 'sea': 2, 'max': 1, 'boy': 1, 'really': 23, 'err': 1, 'grim': 0, 'well': 55, 'look': 35, 'huh': 10, 'sorry': 34, 'familiar': 0, 'see': 68, 'gee': 0, 'whoa': 2, 'careful': 0, 'gonna': 27, 'easy': 2, 'tomorrow': 1, 'like': 71, 'could': 31, 'ariel': 1, 'ok': 0, 'going': 27, 'stranger': 1, 'stay': 8, \"there's\": 16, 'course': 0, 'dinner': 1, 'would': 37, 'join': 0, 'never': 70, 'get': 59, 'show': 1, 'want': 52, 'yet': 0, 'way': 28, 'anything': 8, 'belle': 1, \"that's\": 39, 'ahead': 0, 'take': 9, 'eat': 1, 'gotta': 4, 'jump': 1, 'wish': 17, 'abu': 2, 'hey': 12, 'street': 2, 'rat': 1, 'time': 33, 'uh': 25, 'must': 14, 'ah': 4, 'wishes': 0, 'princess': 15, 'right': 51, 'make': 22, 'jasmine': 0, 'sultan': 3, 'genie': 0, 'tell': 28, 'got': 39, 'dad': 6, 'yeah': 9, 'okay': 1, 'king': 10, 'great': 8, 'think': 26, 'scar': 3, 'let': 62, 'better': 8, 'help': 29, 'mean': 31, \"we're\": 6, 'life': 23}, 'non-p': {'slave': 0, 'magic': 1, 'mirror': 0, 'come': 74, 'farthest': 0, 'space': 0, 'wind': 0, 'darkness': 0, 'summon': 0, 'thee': 0, 'speak': 0, 'let': 62, 'see': 68, 'thy': 0, 'face': 0, 'wouldst': 0, 'thou': 0, 'know': 104, 'queen': 2, 'wall': 0, 'fairest': 0, 'one': 51, 'famed': 0, 'beauty': 0, 'majesty': 1, 'hold': 0, 'lovely': 0, 'maid': 0, 'rags': 0, 'cannot': 0, 'hide': 0, 'gentle': 0, 'grace': 0, 'alas': 0, 'fair': 0, 'reveal': 0, 'name': 0, 'lips': 0, 'red': 0, 'rose': 0, 'hair': 0, 'black': 0, 'ebony': 0, 'skin': 0, 'white': 1, 'snow': 1, 'take': 21, 'far': 0, 'forest': 0, 'find': 4, 'yes': 43, 'fail': 0, \"can't\": 57, 'forgive': 0, 'run': 1, 'go': 85, 'back': 43, 'dig': 1, 'pick': 0, 'quick': 0, 'mine': 0, 'diamonds': 0, 'heighho': 3, 'home': 1, 'work': 1, 'day': 33, 'sneak': 0, 'shh': 1, 'uh': 25, 'men': 2, 'search': 0, 'look': 35, 'gosh': 0, 'whole': 0, \"there's\": 21, \"ain't\": 0, 'stole': 0, 'washed': 0, \"something's\": 0, 'away': 14, 'ya': 3, 'gotta': 2, \"comin'\": 0, 'stop': 0, 'oh': 188, 'make': 22, 'get': 59, \"that's\": 39, 'right': 51, 'us': 26, 'dragon': 0, 'never': 71, 'kill': 1, \"let's\": 2, \"iit's\": 0, 'yeah': 9, 'hah': 1, 'princess': 15, \"we're\": 6, 'mad': 0, 'bad': 2, 'like': 72, 'wicked': 0, 'well': 55, \"'em\": 0, \"she'll\": 3, 'wash': 5, 'tub': 0, \"'im\": 0, 'mighty': 0, 'good': 27, 'heart': 5, 'old': 6, 'sleeping': 0, 'got': 39, 'hey': 12, 'ah': 10, 'tell': 28, \"we'll\": 12, 'comfortable': 0, 'apple': 1, 'bite': 1, 'song': 7, 'love': 48, 'true': 13, 'cinderella': 0, 'little': 39, 'lucifee': 0, 'zukzuk': 0, 'hup': 0, 'time': 33, 'sire': 0, 'sing': 15, 'mother': 2, 'cinderelly': 0, 'hurry': 4, 'keep': 4, 'dreams': 8, 'dear': 4, 'child': 2, 'pretty': 0, 'slipper': 1, 'say': 34, 'upon': 0, 'prince': 28, 'gusgus': 0, 'shall': 1, 'king': 10, 'course': 4, 'think': 36, 'dress': 5, 'phillip': 0, 'ariel': 1, 'something': 25, 'girl': 9, 'la': 2, 'kiss': 0, 'sea': 2, 'eric': 7, 'way': 28, 'going': 27, 'gaston': 14, 'belle': 1, 'must': 18, 'beast': 3, 'ha': 13, 'boy': 2, 'simba': 14, 'want': 52, 'man': 10, 'gonna': 27}}\n"
     ]
    }
   ],
   "source": [
    "print(populars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
