{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "primary-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/Feb/2022 12:43:25] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return '<html><body><p>жопа хуй!</p></body></html>'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-geography",
   "metadata": {},
   "source": [
    "НУЖНЫЕ ХРЕНИ НАЧИНАЯ ОТСЮДА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acquired-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "impressive-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# чистка от пунктуации\n",
    "\n",
    "punct_list = '!\"«»“”#$%&\\–-–—()*+,./\\:;<=>?@[]^_`{|}~1234567890'\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    for char in text:\n",
    "        if char in punct_list:\n",
    "            text = text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "therapeutic-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops.extend([\"i'm\", \"he's\", \"i've\", \"i'll\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bulgarian-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.extend([\"i'm\", \"he's\", \"i've\", \"i'll\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# списки слов принцесс, принцов и не-пр\n",
    "\n",
    "txt_ps = []\n",
    "txt_pe = []\n",
    "txt_np = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "blessed-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция: на вход строка — на выход список полезных слов (т. е. без стоп-слов и пунктуации)\n",
    "\n",
    "def extract_words(any_row):\n",
    "    words = clean(any_row).split()\n",
    "    good_words = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            good_words.append(word)\n",
    "                \n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "announced-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('princess_corpus.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        row = dict(row)\n",
    "        \n",
    "        # принцессы\n",
    "        if row['Speaker_Status'] == 'PRINCESS':\n",
    "            txt_ps.extend(extract_words(row['Text']))\n",
    "            \n",
    "        with open('princess.txt','a') as new_f:\n",
    "            stri = ' '.join(extract_words(row['Text'])) + '\\n'\n",
    "            new_f.write(stri)\n",
    "            \n",
    "        # принцы\n",
    "        if row['Speaker_Status'] == 'PRINCE':\n",
    "            txt_pe.extend(extract_words(row['Text']))\n",
    "            \n",
    "        # челядь\n",
    "        if row['Speaker_Status'] == 'NON-P':\n",
    "            txt_np.extend(extract_words(row['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eleven-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh \t 376\n",
      "know \t 208\n",
      "go \t 170\n",
      "come \t 148\n",
      "like \t 144\n",
      "never \t 142\n",
      "please \t 136\n",
      "see \t 136\n",
      "let \t 124\n",
      "get \t 118\n",
      "can't \t 114\n",
      "well \t 110\n",
      "want \t 104\n",
      "one \t 102\n",
      "right \t 102\n",
      "love \t 96\n",
      "wait \t 90\n",
      "yes \t 86\n",
      "back \t 86\n",
      "that's \t 78\n",
      "little \t 78\n",
      "got \t 78\n",
      "father \t 76\n",
      "would \t 74\n",
      "think \t 72\n",
      "mom \t 72\n",
      "look \t 70\n",
      "say \t 68\n",
      "sorry \t 68\n",
      "day \t 66\n",
      "time \t 66\n",
      "okay \t 64\n",
      "mean \t 62\n",
      "could \t 62\n",
      "away \t 60\n",
      "elsa \t 60\n",
      "help \t 58\n",
      "tell \t 56\n",
      "way \t 56\n",
      "prince \t 56\n",
      "around \t 56\n",
      "true \t 54\n",
      "going \t 54\n",
      "good \t 54\n",
      "thank \t 54\n",
      "gonna \t 54\n",
      "us \t 52\n",
      "find \t 50\n",
      "mother \t 50\n",
      "uh \t 50\n"
     ]
    }
   ],
   "source": [
    "#считаем популярные слова ПРИНЦЕСС\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "freq_ps = Counter(txt_ps)\n",
    "freq_psSorted = sorted(freq_ps.items(), key = lambda x: x[1], reverse=True)\n",
    "for i in freq_psSorted[:50]:\n",
    "    print(i[0], \"\\t\", freq_ps[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "incorporated-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right \t 49\n",
      "well \t 44\n",
      "know \t 43\n",
      "one \t 42\n",
      "come \t 42\n",
      "get \t 42\n",
      "oh \t 39\n",
      "go \t 38\n",
      "can't \t 36\n",
      "like \t 36\n",
      "see \t 31\n",
      "look \t 31\n",
      "wait \t 29\n",
      "hey \t 26\n",
      "way \t 25\n",
      "abu \t 25\n",
      "yes \t 24\n",
      "really \t 24\n",
      "dad \t 24\n",
      "that's \t 23\n",
      "back \t 22\n",
      "yeah \t 22\n",
      "gonna \t 21\n",
      "never \t 20\n",
      "want \t 20\n",
      "uh \t 20\n",
      "would \t 19\n",
      "please \t 18\n",
      "prince \t 18\n",
      "find \t 18\n",
      "princess \t 18\n",
      "time \t 18\n",
      "let \t 18\n",
      "genie \t 18\n",
      "could \t 17\n",
      "make \t 17\n",
      "something \t 16\n",
      "sorry \t 16\n",
      "got \t 16\n",
      "tell \t 16\n",
      "jasmine \t 16\n",
      "mean \t 15\n",
      "king \t 15\n",
      "life \t 15\n",
      "love \t 14\n",
      "help \t 14\n",
      "we're \t 14\n",
      "going \t 13\n",
      "great \t 13\n",
      "huh \t 13\n"
     ]
    }
   ],
   "source": [
    "#считаем популярные слова ПРИНЦЕВ\n",
    "\n",
    "freq_pe = Counter(txt_pe)\n",
    "freq_peSorted = sorted(freq_pe.items(), key = lambda x: x[1], reverse=True)\n",
    "for i in freq_peSorted[:50]:\n",
    "    print(i[0], \"\\t\", freq_pe[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "allied-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh \t 367\n",
      "get \t 212\n",
      "got \t 209\n",
      "like \t 200\n",
      "well \t 198\n",
      "come \t 196\n",
      "one \t 187\n",
      "go \t 178\n",
      "know \t 176\n",
      "yes \t 168\n",
      "right \t 151\n",
      "little \t 149\n",
      "see \t 146\n",
      "look \t 146\n",
      "that's \t 131\n",
      "love \t 129\n",
      "good \t 120\n",
      "time \t 110\n",
      "us \t 109\n",
      "make \t 104\n",
      "prince \t 104\n",
      "back \t 103\n",
      "can't \t 103\n",
      "take \t 102\n",
      "princess \t 97\n",
      "way \t 97\n",
      "let \t 96\n",
      "girl \t 95\n",
      "say \t 95\n",
      "uh \t 93\n",
      "think \t 90\n",
      "gonna \t 88\n",
      "king \t 87\n",
      "we're \t 85\n",
      "hey \t 83\n",
      "want \t 81\n",
      "dear \t 79\n",
      "going \t 77\n",
      "ah \t 76\n",
      "yeah \t 75\n",
      "man \t 75\n",
      "ha \t 75\n",
      "dig \t 74\n",
      "old \t 71\n",
      "never \t 70\n",
      "must \t 69\n",
      "day \t 68\n",
      "there's \t 67\n",
      "something \t 67\n",
      "tell \t 66\n"
     ]
    }
   ],
   "source": [
    "#считаем популярные слова НЕ-П\n",
    "\n",
    "freq_np = Counter(txt_np)\n",
    "freq_npSorted = sorted(freq_np.items(), key = lambda x: x[1], reverse=True)\n",
    "for i in freq_npSorted[:50]:\n",
    "    print(i[0], \"\\t\", freq_np[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта функция, которая принимает на вход датассет\n",
    "# и для каждой строки возвращает список полезных слов без пунктуации\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops.extend([\"i'm\", \"he's\", \"i've\", \"i'll\"])\n",
    "\n",
    "# читска от пунктуации\n",
    "punct_list = '!\"«»“”#$%&\\–-–—()*+,./\\:;<=>?@[]^_`{|}~1234567890'\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    for char in text:\n",
    "        if char in punct_list:\n",
    "            text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# функция находящая полезные слова\n",
    "def extract_words(dataset):\n",
    "    df = pd.read_csv(dataset)\n",
    "    quotes = df['Text'].values\n",
    "    \n",
    "    for quot in quotes:\n",
    "        words = clean(quot).split()\n",
    "        good_words = []\n",
    "        for word in words:\n",
    "            if word not in stops:\n",
    "                good_words.append(word)\n",
    "                \n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-rolling",
   "metadata": {},
   "source": [
    "Конец полезной фигни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "rolled-geology",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-2311dd8a8256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "excellent-scoop",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-12102ff9e1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m     51\u001b[0m     return util.load_model(\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "worse-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement en_core_web_sm\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for en_core_web_sm\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "incorporate-wheel",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-76-81fd96c65d18>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-76-81fd96c65d18>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    python -m spacy download en_core_web_sm\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "на вход реплика"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
